{
    "model_name": "Snowflake/snowflake-arctic-embed-l-v2.0",
    "timestamp": "2025-05-08 14:38:45",
    "device": "cuda",
    "test_sentence_used": "This benchmark evaluates the inference speed and memory footprint of various sentence embedding models when processing a large volume of repeated textual data on different hardware configurations.",
    "prefix_function_name": "query",
    "normalize_embeddings_in_encode_call": false,
    "num_sentences_processed": 100000,
    "batch_size": 64,
    "warmup_runs": 256,
    "ram_psutil_before_load_mb": 469.82,
    "ram_resource_maxrss_before_load_mb": 469.82,
    "effective_device": "cuda",
    "gpu_mem_before_load": {
        "gpu_memory_allocated_mb": 0.0,
        "gpu_memory_peak_allocated_mb": 0.0,
        "gpu_memory_reserved_mb": 0.0,
        "gpu_memory_peak_reserved_mb": 0.0
    },
    "model_load_time_s": 21.32,
    "ram_psutil_after_load_mb": 892.72,
    "model_ram_footprint_psutil_approx_mb": 422.9,
    "ram_resource_maxrss_after_load_mb": 3058.08,
    "gpu_mem_after_load": {
        "gpu_memory_allocated_mb": 2165.94,
        "gpu_memory_peak_allocated_mb": 2165.94,
        "gpu_memory_reserved_mb": 2186.0,
        "gpu_memory_peak_reserved_mb": 2186.0
    },
    "ram_psutil_after_warmup_mb": 1029.73,
    "ram_resource_maxrss_after_warmup_mb": 3058.08,
    "gpu_mem_after_warmup": {
        "gpu_memory_allocated_mb": 2174.06,
        "gpu_memory_peak_allocated_mb": 2300.41,
        "gpu_memory_reserved_mb": 2376.0,
        "gpu_memory_peak_reserved_mb": 2376.0
    },
    "output_embedding_dim": 1024,
    "inference_time_s": 84.9,
    "ram_psutil_after_inference_mb": 1854.73,
    "ram_resource_maxrss_after_inference_mb": 3058.08,
    "gpu_mem_after_inference": {
        "gpu_memory_allocated_mb": 2174.06,
        "gpu_memory_peak_allocated_mb": 2300.41,
        "gpu_memory_reserved_mb": 2376.0,
        "gpu_memory_peak_reserved_mb": 2376.0
    },
    "sentences_per_second": 1177.86
}