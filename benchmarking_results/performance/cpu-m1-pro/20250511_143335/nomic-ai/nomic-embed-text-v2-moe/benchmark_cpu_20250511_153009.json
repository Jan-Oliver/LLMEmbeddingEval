{
    "model_name": "nomic-ai/nomic-embed-text-v2-moe",
    "timestamp": "2025-05-11 15:25:52",
    "device": "cpu",
    "test_sentence_used": "This benchmark evaluates the inference speed and memory footprint of various sentence embedding models when processing a large volume of repeated textual data on different hardware configurations.",
    "prefix_function_name": "none",
    "normalize_embeddings_in_encode_call": false,
    "num_sentences_processed": 15000,
    "batch_size": 64,
    "warmup_runs": 256,
    "ram_psutil_before_load_mb": 376.16,
    "ram_resource_maxrss_before_load_mb": 376.69,
    "effective_device": "cpu",
    "model_load_time_s": 12.1,
    "ram_psutil_after_load_mb": 681.59,
    "model_ram_footprint_psutil_approx_mb": 305.43,
    "ram_resource_maxrss_after_load_mb": 3019.55,
    "ram_psutil_after_warmup_mb": 1787.83,
    "ram_resource_maxrss_after_warmup_mb": 3019.55,
    "output_embedding_dim": 768,
    "inference_time_s": 240.66,
    "ram_psutil_after_inference_mb": 1747.58,
    "ram_resource_maxrss_after_inference_mb": 3019.55,
    "sentences_per_second": 62.33
}